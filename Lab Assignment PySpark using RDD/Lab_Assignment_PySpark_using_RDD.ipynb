{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#PRINOM MOJUMDER\n",
        "#2021-2-60-098"
      ],
      "metadata": {
        "id": "S39KJozm_49Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"RDD Example\").getOrCreate() # making a new spark session. # obj name: spark\n",
        "\n"
      ],
      "metadata": {
        "id": "qeMvMyhZrewy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.\n",
        "Create and Transform an RDD:\n",
        "• Create an RDD from a list of integers.\n",
        "• Perform the following:\n",
        "   Multiply each number by 3 using map.\n",
        "   Filter the numbers greater than 10 using filter.\n",
        "   Collect and display the final result."
      ],
      "metadata": {
        "id": "3J49rd89JoWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "16-eLeq-m6_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtmLQghzJegU",
        "outputId": "f6d3f9d7-9100-4fa6-f0c1-9d8857bd8dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_01_Prinom:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 6, 9, 12, 15, 18, 21, 24, 27]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "rdd_prinom = spark.sparkContext.parallelize([1, 2, 3, 4, 5,6,7,8,9])\n",
        "mapped_rdd_prinom = rdd_prinom.map(lambda x: x * 3)\n",
        "print(\"Task_01_Prinom:\")\n",
        "mapped_rdd_prinom.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_rdd = mapped_rdd_prinom.filter(lambda x: x > 10)\n",
        "print(\"Task_01_Prinom:\")\n",
        "filtered_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAtdcx7DrgWV",
        "outputId": "e43faed6-5496-4f0f-d0e5-5b1537192b91"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_01_Prinom:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 15, 18, 21, 24, 27]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.\n",
        "Read a Text File:\n",
        "  Use textFile to read a file containing sentences (e.g., \"hello world\", \"spark is great\").\n",
        "\n",
        "   Perform the following:\n",
        "     Split each line into words using flatMap.\n",
        "      Count the total number of words using count"
      ],
      "metadata": {
        "id": "Xb_f_tUqr3YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_prinom = spark.sparkContext.textFile(\"/content/RDD.txt\")\n",
        "words = rdd_prinom.flatMap(lambda line: line.split(\" \"))\n",
        "print(\"Task_02_Prinom:\")\n",
        "print(words.collect())\n",
        "words.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD-ZJyfIsy7B",
        "outputId": "e26cbf27-5ce1-440f-d39f-55683226a35a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_02_Prinom:\n",
            "['hello', 'world', 'spark', 'is', 'great']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 GroupByKey and ReduceByKey:\n",
        "\n",
        "• Create an RDD with pairs of student names and their scores, e.g., [(\"Alice\", 85),\n",
        "(\"Bob\", 90), (\"Alice\", 95)].\n",
        "\n",
        "\n",
        "• Use:\n",
        "o groupByKey to group scores by student.\n",
        "\n",
        "o reduceByKey to calculate the total score for each student."
      ],
      "metadata": {
        "id": "if14Bi3UulDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_prinom = spark.sparkContext.parallelize([(\"Alice\", 85), (\"Bob\", 90), (\"Alice\", 95)])\n",
        "grouped_rdd = rdd_prinom.groupByKey()\n",
        "grouped_result = [(k, list(v)) for k, v in grouped_rdd.collect()]\n",
        "print(\"Task_03_Prinom:\")\n",
        "print(grouped_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dhYF3DLryAN",
        "outputId": "14272523-254a-4efd-e69b-bd47768dda12"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_03_Prinom:\n",
            "[('Alice', [85, 95]), ('Bob', [90])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_rdd  = rdd_prinom.reduceByKey(lambda x, y: x + y)\n",
        "reduced_result = reduced_rdd.collect()\n",
        "print(reduced_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsyh-XLhvAKd",
        "outputId": "ea0f29ad-c3c6-4ab0-f3bf-8a27fe139add"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', 180), ('Bob', 90)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. RDD Persistence:\n",
        "• Create an RDD from a large list (simulate it by generating numbers from 1 to 1,000,000).\n",
        "\n",
        "• Perform multiple actions (e.g., count, sum) without caching and measure execution\n",
        "time.\n",
        "\n",
        "• Repeat with cache or persist and compare the performance."
      ],
      "metadata": {
        "id": "CGzBbHMpwUFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Task_04_prinom:\")\n",
        "import time\n",
        "# Without cache\n",
        "rdd_prinom = spark.sparkContext.parallelize(range(1, 1000001)) #1 to 1000000\n",
        "start_time = time.time()\n",
        "\n",
        "count_prinom = rdd_prinom.count()\n",
        "sum_prinom = rdd_prinom.sum()\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Without Cache:\")\n",
        "print(f\"Count: {count_prinom}, Sum: {sum_prinom}\")\n",
        "print(f\"Time: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtisCSVmwRQL",
        "outputId": "b1cd4347-b7e6-404b-e851-d6995ac055a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_04_prinom:\n",
            "Without Cache:\n",
            "Count: 1000000, Sum: 500000500000\n",
            "Time: 1.0604298114776611 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With Cache\n",
        "rdd_prinom_cached = rdd_prinom.cache()\n",
        "start_time = time.time()\n",
        "count_prinom_cached = rdd_prinom_cached.count()\n",
        "sum_prinom_cached = rdd_prinom_cached.sum()\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"With Cache:\")\n",
        "print(f\"Count: {count_prinom_cached}, Sum: {sum_prinom_cached}\")\n",
        "print(f\"Time: {end_time - start_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXhwvGWdy48y",
        "outputId": "5fae1bc8-55f7-4b42-f4c7-4e944c9e61c9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With Cache:\n",
            "Count: 1000000, Sum: 500000500000\n",
            "Time: 2.1086349487304688 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Custom Transformations:\n",
        "\n",
        "• Create an RDD of numbers.\n",
        "\n",
        "• Write a custom transformation to identify and filter out prime numbers."
      ],
      "metadata": {
        "id": "5m1Ysg3iyp2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_prinom = spark.sparkContext.parallelize(range(1, 101))\n",
        "\n",
        "def prime(n):\n",
        "    if n <= 1:\n",
        "        return False\n",
        "    for i in range(2, n):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "rdd = rdd_prinom.filter(prime)\n",
        "print(\"Task_05_prinom:\")\n",
        "print(rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkOUvApHyg5u",
        "outputId": "06f9b16d-750c-4a29-cdcb-3eb701a3e0a8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_05_prinom:\n",
            "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd_prinom = spark.sparkContext.parallelize(range(1, 101))\n",
        "\n",
        "# Use lambda for prime number:\n",
        "rdd = rdd_prinom.filter(lambda n: n > 1 and all(n % i != 0 for i in range(2, n)))\n",
        "\n",
        "print(\"Task_05_prinom:\")\n",
        "print(rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQ9_pny0Us7",
        "outputId": "9025546a-7bce-4e3d-e1fd-aae9b53b0509"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_05_prinom:\n",
            "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Transformation and Action Workflow:\n",
        "• Load a dataset (e.g., CSV or text file) containing the following:\n",
        "\n",
        "[Product,Category,Price\n",
        "\n",
        "Laptop,Electronics,800\n",
        "\n",
        "Shoes,Clothing,50\n",
        "\n",
        "Phone,Electronics,500]\n",
        "_________________________________________________________________________________\n",
        "\n",
        "• Perform the following:\n",
        "\n",
        "o Filter products with a price greater than 100.\n",
        "\n",
        "o Map to get only the product names.\n",
        "\n",
        "o Count the number of products in each category using map and reduceByKey."
      ],
      "metadata": {
        "id": "inJQx2r128qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    \"Laptop,Electronics,800\",\n",
        "    \"Shoes,Clothing,50\",\n",
        "    \"Phone,Electronics,500\"\n",
        "]\n",
        "rdd = spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "oqhCkQ4B2QrA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_rdd = rdd.filter(lambda line: int(line.split(\",\")[2]) > 100)\n",
        "\n",
        "product_names_rdd = filtered_rdd.map(lambda line: line.split(\",\")[0])\n",
        "\n",
        "category_counts_rdd = (\n",
        "    rdd.map(lambda line: (line.split(\",\")[1], 1))\n",
        "    .reduceByKey(lambda x, y: x + y)\n",
        ")\n",
        "\n",
        "print(\"Task_06_prinom:\")\n",
        "print(\"Filtered Products:\")\n",
        "print(filtered_rdd.collect())\n",
        "\n",
        "print(\"\\nProduct Names:\")\n",
        "print(product_names_rdd.collect())\n",
        "\n",
        "print(\"\\nCategory Counts:\")\n",
        "print(category_counts_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6FlQ54v4U49",
        "outputId": "cd7d8db1-32d3-42f2-8e92-9ef5a8f0c4a3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_06_prinom:\n",
            "Filtered Products:\n",
            "['Laptop,Electronics,800', 'Phone,Electronics,500']\n",
            "\n",
            "Product Names:\n",
            "['Laptop', 'Phone']\n",
            "\n",
            "Category Counts:\n",
            "[('Electronics', 2), ('Clothing', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Integration with Spark SQL:\n",
        "\n",
        "• Load a JSON dataset containing information about students:\n",
        "\n",
        "[{\"name\": \"Alice\", \"age\": 20, \"grade\": \"A\"},\n",
        "{\"name\": \"Bob\", \"age\": 22, \"grade\": \"B\"}]\n",
        "____________________________________________________________________________\n",
        "• Perform the following:\n",
        "\n",
        "o Register the data as a temporary SQL\n",
        " table.\n",
        "\n",
        "o Query students who have a grade \"A\" using Spark SQL.\n",
        "\n",
        "o Save the result to a new JSON file."
      ],
      "metadata": {
        "id": "1PlqsbyZ6IoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Task_07_prinom:\")\n",
        "rdd_prinom = spark.sparkContext.parallelize([{\"name\": \"Alice\", \"age\": 20, \"grade\": \"A\"},\n",
        "    {\"name\": \"Bob\", \"age\": 22, \"grade\": \"B\"}\n",
        "])\n",
        "df_prinom = spark.read.json(rdd_prinom)\n",
        "\n",
        "print(\"SQL Table:\")\n",
        "df_prinom.createOrReplaceTempView(\"students\")\n",
        "df_prinom.show()\n",
        "\n",
        "print(\"\\nStudents who have a grade A:\")\n",
        "result_prinom = spark.sql(\"SELECT * FROM students WHERE grade = 'A'\")\n",
        "result_prinom.show()\n",
        "\n",
        "result_prinom.write.json(\"outputJ_GradeA.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNj62uW849ds",
        "outputId": "7f0e96a8-9f38-4de9-b115-198b043849b4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_07_prinom:\n",
            "SQL Table:\n",
            "+---+-----+-----+\n",
            "|age|grade| name|\n",
            "+---+-----+-----+\n",
            "| 20|    A|Alice|\n",
            "| 22|    B|  Bob|\n",
            "+---+-----+-----+\n",
            "\n",
            "\n",
            "Students who have a grade A:\n",
            "+---+-----+-----+\n",
            "|age|grade| name|\n",
            "+---+-----+-----+\n",
            "| 20|    A|Alice|\n",
            "+---+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Advanced Word Count with Sorting:\n",
        "\n",
        "• Extend the word count program to:\n",
        "\n",
        "o Sort words by their frequency in descending order.\n",
        "\n",
        "o Display the top 5 most frequent words."
      ],
      "metadata": {
        "id": "Y7GfT44r7wD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Task_08_Prinom:\")\n",
        "rdd_Prinom = spark.sparkContext.parallelize([\n",
        "    \"Hello EWU\",\n",
        "    \"Prinom loves EWU\",\n",
        "    \"Prinom loves CSE of EWU\"\n",
        "])\n",
        "words = rdd_Prinom.flatMap(lambda line: line.split(\" \"))\n",
        "word_counts = words.map(lambda word: (word.lower(), 1))\n",
        "\n",
        "word_counts_total = word_counts.reduceByKey(lambda x, y: x + y)\n",
        "sorted_word_counts = word_counts_total.sortBy(lambda x: x[1], ascending=False)\n",
        "top_5 = sorted_word_counts.take(5)\n",
        "\n",
        "print(\"Top 5 Most Frequent Words:\")\n",
        "for word, count in top_5:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9G8_YzK7bqH",
        "outputId": "7eaba8cf-0cb1-407d-ce36-f505bed18067"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_08_Prinom:\n",
            "Top 5 Most Frequent Words:\n",
            "ewu: 3\n",
            "loves: 2\n",
            "prinom: 2\n",
            "cse: 1\n",
            "of: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Custom Aggregations with aggregateByKey:\n",
        "\n",
        "• Create an RDD with pairs of cities and temperatures, e.g., [(\"NY\", 32), (\"LA\",\n",
        "75), (\"NY\", 28)].\n",
        "\n",
        "• Use aggregateByKey to calculate the average temperature for each city."
      ],
      "metadata": {
        "id": "mdQQ17IR8mpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"NY\", 32), (\"LA\", 75), (\"NY\", 28)]\n",
        "rdd_PM_9 = spark.sparkContext.parallelize(data)\n",
        "\n",
        "aggregated_rdd_PM_9 = rdd_PM_9.aggregateByKey(\n",
        "    (0, 0),\n",
        "    lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
        "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        ")\n",
        "\n",
        "average_rdd_PM_9 = aggregated_rdd_PM_9.mapValues(lambda x: x[0] / x[1])\n",
        "\n",
        "results = average_rdd_PM_9.collect()\n",
        "print(\"Task_09_Prinom:\")\n",
        "print(\"Average Temperature for Each City:\")\n",
        "for city, avg_temp in results:\n",
        "    print(f\"City: {city}, Average Temperature: {avg_temp:.2f}\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJdE3RRZ86DF",
        "outputId": "bfc2d09b-0426-4ec8-d197-90e225f7d0a6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task_09_Prinom:\n",
            "Average Temperature for Each City:\n",
            "City: NY, Average Temperature: 30.00\n",
            "City: LA, Average Temperature: 75.00\n"
          ]
        }
      ]
    }
  ]
}